{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a sentiment analyzer to classify the sentiment of a twitter post as positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Standford labelled twitter data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(\"/Users/Monika/Documents/Apache_Spark/spark-3.1.1-bin-hadoop2.7\")\n",
    "\n",
    "#ref: https://kevinvecmanis.io/python/pyspark/install/2019/05/31/Installing-Apache-Spark.html\n",
    "#'Users/vanaurum/server/spark-2.4.3-bin-hadoop2.7'\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import format_number as fmt\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer,Word2Vec,StopWordsRemover,StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AAPL labelled tweets data: https://data.world/crowdflower/apple-twitter-sentiment\n",
    "\n",
    "#https://stackoverflow.com/questions/2272908/too-many-open-files-how-many-are-open-what-they-are-and-how-many-can-the-jvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the data from tar file\n",
    "\n",
    "with zipfile.ZipFile(\"training.1600000.processed.noemoticon.csv.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName('Sentiment_Analyser')\n",
    "    .config('spark.executor.memory', \"1G\") \n",
    "    .config('spark.driver.memory', \"2G\") \n",
    "    .config('spark.sql.shuffle.partitions',2) \n",
    "    .config('spark.worker.cleanup.enabled', 'True')\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\")\n",
    "    .getOrCreate())\n",
    "\n",
    "#ref: https://stackoverflow.com/questions/61328134/standalone-pyspark-error-too-many-open-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled Tweets count is:  1600000\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from csv file to spark dataframe\n",
    "\n",
    "data = spark.read.csv(\"data/training.1600000.processed.noemoticon.csv\")\n",
    "\n",
    "data = data.dropna()\n",
    "data = data.dropDuplicates()\n",
    "print(\"Labelled Tweets count is: \", data.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|  0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|  0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|  0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|  0|1467813985|Mon Apr 06 22:20:...|NO_QUERY|         quanvu|@alydesigns i was...|\n",
      "|  0|1467813992|Mon Apr 06 22:20:...|NO_QUERY|     swinspeedx|one of my friend ...|\n",
      "|  0|1467814180|Mon Apr 06 22:20:...|NO_QUERY|     viJILLante|this week is not ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed(\"_c0\",\"sentiment_value\")\n",
    "data = data.withColumnRenamed(\"_c5\",\"tweet_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------+-------+--------------------+\n",
      "|sentiment_value|       _c1|                 _c2|     _c3|    _c4|       tweet_content|\n",
      "+---------------+----------+--------------------+--------+-------+--------------------+\n",
      "|              0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|mybirch|         Need a hug |\n",
      "|              0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|   coZZ|@LOLTrish hey  lo...|\n",
      "+---------------+----------+--------------------+--------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|sentiment_value|       tweet_content|\n",
      "+---------------+--------------------+\n",
      "|              0|         Need a hug |\n",
      "|              0|@LOLTrish hey  lo...|\n",
      "|              0|@Tatiana_K nope t...|\n",
      "|              0|spring break in p...|\n",
      "|              0|@caregiving I cou...|\n",
      "|              0|@iamjazzyfizzle I...|\n",
      "|              0|Hollis' death sce...|\n",
      "|              0|@alydesigns i was...|\n",
      "|              0|one of my friend ...|\n",
      "|              0|this week is not ...|\n",
      "+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"sentiment_value\",\"tweet_content\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"sentiment_value\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sentiment_value|\n",
      "+---------------+\n",
      "|              4|\n",
      "|              0|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"sentiment_value\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|sentiment_value| count|\n",
      "+---------------+------+\n",
      "|              4|800000|\n",
      "|              0|800000|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"sentiment_value\").groupBy(\"sentiment_value\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|sentiment_value| count|\n",
      "+---------------+------+\n",
      "|              1|800000|\n",
      "|              0|800000|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"sentiment_value\",regexp_replace(\"sentiment_value\",\"4\",\"1\")) #ref:https://dwgeek.com/replace-pyspark-dataframe-column-value-methods.html/\n",
    "data.groupBy(\"sentiment_value\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------+---------------+--------------------+\n",
      "|sentiment_value|       _c1|                 _c2|     _c3|            _c4|       tweet_content|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+\n",
      "|              0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|              0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|              0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pattern(text, pattern):\n",
    "    \"\"\"\n",
    "    Function to identify the given pattern in a string and remove all the occurences\n",
    "    \"\"\"\n",
    "    r = re.findall(pattern, text)\n",
    "    for i in r:\n",
    "        text = re.sub(i, '', text)        \n",
    "    return text\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \n",
    "    \"\"\"\n",
    "      Function to remove tweet mentions, retweet tags, URL's, numbers and special characters\n",
    "    \"\"\"\n",
    "    text = clean_pattern(text, 'RT @[\\w]*:')\n",
    "\n",
    "    text = clean_pattern(text, '@[\\w]*')\n",
    "    \n",
    "    text = clean_pattern(text, 'https?://[A-Za-z0-9./]*')\n",
    "\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet_udf = udf(clean_tweet,StringType())\n",
    "\n",
    "data = data.withColumn(\"cleaned_tweet\", clean_tweet_udf(\"tweet_content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       tweet_content|       cleaned_tweet|\n",
      "+--------------------+--------------------+\n",
      "|         Need a hug |         Need a hug |\n",
      "|@LOLTrish hey  lo...| hey long time no...|\n",
      "|@Tatiana_K nope t...| nope they didn t...|\n",
      "|spring break in p...|spring break in p...|\n",
      "|@caregiving I cou...| I couldn t bear ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"tweet_content\",\"cleaned_tweet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"label\",data[\"sentiment_value\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data,test_data = data.randomSplit([0.7,0.3])\n",
    "\n",
    "#Performing split such that equal proportion of labels are present in both train and test datasets\n",
    "\n",
    "#ref: https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark\n",
    "# split dataframes between 0s and 1s\n",
    "zeros = data.filter(data[\"sentiment_value\"]==0)\n",
    "ones = data.filter(data[\"sentiment_value\"]==1)\n",
    "\n",
    "# split datasets into training and testing\n",
    "train0, test0 = zeros.randomSplit([0.8,0.2])\n",
    "train1, test1 = ones.randomSplit([0.8,0.2])\n",
    "\n",
    "# stack datasets back together\n",
    "train_data = train0.union(train1)\n",
    "test_data = test0.union(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|sentiment_value| count|\n",
      "+---------------+------+\n",
      "|              1|640292|\n",
      "|              0|640266|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby(\"sentiment_value\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words Removal and Vectorisation of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='cleaned_tweet', outputCol='tokenized_tweet')\n",
    "\n",
    "#As this model is a sentiment analyser, removing the \"not\" word from the stop words list\n",
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol='tokenized_tweet',outputCol='cleaned_tokens',stopWords=stop_words)\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=5, inputCol='cleaned_tokens', outputCol='features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline= Pipeline(stages=[tokenizer,stopword_remover,word2Vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation of logistic regression, random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vectorised = data_cleaning_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|sentiment_value|       _c1|                 _c2|     _c3|            _c4|       tweet_content|       cleaned_tweet|label|     tokenized_tweet|      cleaned_tokens|            features|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|              0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |         Need a hug |    0|      [need, a, hug]|         [need, hug]|[0.25837088376283...|\n",
      "|              0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...| nope they didn t...|    0|[, nope, they, di...|      [, nope, didn]|[-0.1228572515149...|\n",
      "|              0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|spring break in p...|    0|[spring, break, i...|[spring, break, p...|[-0.0721139837056...|\n",
      "|              0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...| I wish I got to ...|    0|[, i, wish, i, go...|[, wish, got, wat...|[0.00867272897933...|\n",
      "|              0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|Hollis death scen...|    0|[hollis, death, s...|[hollis, death, s...|[-0.0866188507061...|\n",
      "|              0|1467813985|Mon Apr 06 22:20:...|NO_QUERY|         quanvu|@alydesigns i was...| i was out most o...|    0|[, i, was, out, m...|[, day, didn, get...|[0.01925909146666...|\n",
      "|              0|1467813992|Mon Apr 06 22:20:...|NO_QUERY|     swinspeedx|one of my friend ...|one of my friend ...|    0|[one, of, my, fri...|[one, friend, cal...|[0.05948686371134...|\n",
      "|              0|1467814180|Mon Apr 06 22:20:...|NO_QUERY|     viJILLante|this week is not ...|this week is not ...|    0|[this, week, is, ...|[week, going, hoped]|[0.01305209907392...|\n",
      "|              0|1467814783|Mon Apr 06 22:20:...|NO_QUERY|    KatieAngell|Just going to cry...|Just going to cry...|    0|[just, going, to,...|[going, cry, slee...|[-0.0167350441217...|\n",
      "|              0|1467815199|Mon Apr 06 22:20:...|NO_QUERY|        abel209|ooooh.... LOL  th...|ooooh LOL that le...|    0|[ooooh, lol, that...|[ooooh, lol, lesl...|[0.09470003491474...|\n",
      "|              0|1467815753|Mon Apr 06 22:21:...|NO_QUERY|BaptisteTheFool|Meh... Almost Lov...|Meh Almost Lover ...|    0|[meh, almost, lov...|[meh, almost, lov...|[0.00599261942423...|\n",
      "|              0|1467815924|Mon Apr 06 22:21:...|NO_QUERY|          EmCDL|@alielayus I want...| I want to go to ...|    0|[, i, want, to, g...|[, want, go, prom...|[0.05911690592765...|\n",
      "|              0|1467815988|Mon Apr 06 22:21:...|NO_QUERY|       merisssa|thought sleeping ...|thought sleeping ...|    0|[thought, sleepin...|[thought, sleepin...|[0.10394782448808...|\n",
      "|              0|1467816665|Mon Apr 06 22:21:...|NO_QUERY|           jsoo|@HumpNinja I cry ...| I cry my asian e...|    0|[, i, cry, my, as...|[, cry, asian, ey...|[0.05467921247084...|\n",
      "|              0|1467817225|Mon Apr 06 22:21:...|NO_QUERY|    crosland_12|@cocomix04 ill te...| ill tell ya the ...|    0|[, ill, tell, ya,...|[, ill, tell, ya,...|[0.06492603527238...|\n",
      "|              0|1467817374|Mon Apr 06 22:21:...|NO_QUERY|        ajaxpro|@MissXu sorry! be...| sorry bed time c...|    0|[, sorry, bed, ti...|[, sorry, bed, ti...|[0.03432768334945...|\n",
      "|              0|1467817502|Mon Apr 06 22:21:...|NO_QUERY|        Tmttq86|@fleurylis I don'...| I don t either I...|    0|[, i, don, t, eit...|[, either, depres...|[-0.0571408855418...|\n",
      "|              0|1467818603|Mon Apr 06 22:21:...|NO_QUERY|      kennypham|Sad, sad, sad. I ...|Sad sad sad I don...|    0|[sad, sad, sad, i...|[sad, sad, sad, k...|[0.00943180886355...|\n",
      "|              0|1467819650|Mon Apr 06 22:22:...|NO_QUERY|      antzpantz|@Viennah Yay! I'm...| Yay I m happy fo...|    0|[, yay, i, m, hap...|[, yay, m, happy,...|[0.02067816091908...|\n",
      "|              0|1467820863|Mon Apr 06 22:22:...|NO_QUERY|         tautao|Broadband plan 'a...|Broadband plan a ...|    0|[broadband, plan,...|[broadband, plan,...|[0.06025823606894...|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_vectorised.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_labelled_StringIdx = labelled_StringIdx.fit(train_data_vectorised)\n",
    "train_data_vectorised = train_labelled_StringIdx.transform(train_data_vectorised)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data_vectorised.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vectorised = data_cleaning_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|sentiment_value|       _c1|                 _c2|     _c3|            _c4|       tweet_content|       cleaned_tweet|label|     tokenized_tweet|      cleaned_tokens|            features|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|              0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...| hey long time no...|    0|[, hey, long, tim...|[, hey, long, tim...|[0.05560346074903...|\n",
      "|              0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...| I couldn t bear ...|    0|[, i, couldn, t, ...|[, couldn, bear, ...|[-0.0161670572124...|\n",
      "|              0|1467819812|Mon Apr 06 22:22:...|NO_QUERY|      IrisJumbe|Oh man...was iron...|Oh man was ironin...|    0|[oh, man, was, ir...|[oh, man, ironing...|[0.01970249810256...|\n",
      "|              0|1467821715|Mon Apr 06 22:22:...|NO_QUERY|         deelau|@andywana Not sur...| Not sure what th...|    0|[, not, sure, wha...|[, sure, pos, muc...|[0.03242030080694...|\n",
      "|              0|1467822384|Mon Apr 06 22:22:...|NO_QUERY|    Lindsey0920|@oanhLove I hate ...| I hate when that...|    0|[, i, hate, when,...|   [, hate, happens]|[-0.1298839425047...|\n",
      "|              0|1467822389|Mon Apr 06 22:22:...|NO_QUERY|     HybridMink|I have a sad feel...|I have a sad feel...|    0|[i, have, a, sad,...|[sad, feeling, da...|[0.02776496097600...|\n",
      "|              0|1467822519|Mon Apr 06 22:22:...|NO_QUERY|        gzacher|Ugh....92 degrees...|Ugh degrees tomor...|    0|[ugh, degrees, to...|[ugh, degrees, to...|[0.04406802263110...|\n",
      "|              0|1467825883|Mon Apr 06 22:23:...|NO_QUERY|         deelau|Gym attire today ...|Gym attire today ...|    0|[gym, attire, tod...|[gym, attire, tod...|[0.00561676215147...|\n",
      "|              0|1467835577|Mon Apr 06 22:26:...|NO_QUERY|      viviana09|wednesday my b-da...|wednesday my b da...|    0|[wednesday, my, b...|[wednesday, b, da...|[0.16895569209009...|\n",
      "|              0|1467836111|Mon Apr 06 22:26:...|NO_QUERY|    perrohunter|@makeherfamous hm...| hmm do u really ...|    0|[, hmm, do, u, re...|[, hmm, u, really...|[0.05667676181138...|\n",
      "|              0|1467837602|Mon Apr 06 22:26:...|NO_QUERY|        GetGary|@paradisej cool, ...| cool i will thei...|    0|[, cool, i, will,...|[, cool, kinds, c...|[0.01303193159401...|\n",
      "|              0|1467839007|Mon Apr 06 22:27:...|NO_QUERY|         eyezup|@mercedesashley D...| Damn The grind i...|    0|[, damn, the, gri...|[, damn, grind, i...|[0.04390850176031...|\n",
      "|              0|1467839450|Mon Apr 06 22:27:...|NO_QUERY|  BreannaBonana|ugh. cant sleep. ...|ugh cant sleep it...|    0|[ugh, cant, sleep...|  [ugh, cant, sleep]|[-0.0362304846445...|\n",
      "|              0|1467839586|Mon Apr 06 22:27:...|NO_QUERY|      sonyolmos|@eRRe_sC aaw i mi...| aaw i miss ya al...|    0|[, aaw, i, miss, ...|[, aaw, miss, ya,...|[0.07940015490902...|\n",
      "|              0|1467841885|Mon Apr 06 22:27:...|NO_QUERY|    MissPassion|@thecoolestout Eh...| Ehhh don t Weath...|    0|[, ehhh, don, t, ...|[, ehhh, weather,...|[0.06918735057115...|\n",
      "|              0|1467842568|Mon Apr 06 22:28:...|NO_QUERY|josiahmcdermott|i miss kenny powers |i miss kenny powers |    0|[i, miss, kenny, ...|[miss, kenny, pow...|[0.00623510622729...|\n",
      "|              0|1467843734|Mon Apr 06 22:28:...|NO_QUERY|      chatpataa|my nokia 1110 die...|      my nokia died |    0|   [my, nokia, died]|       [nokia, died]|[0.03055888414382...|\n",
      "|              0|1467844540|Mon Apr 06 22:28:...|NO_QUERY|      ceironous|HEROES just isn't...|HEROES just isn t...|    0|[heroes, just, is...|[heroes, isn, sea...|[-0.2572818398475...|\n",
      "|              0|1467853431|Mon Apr 06 22:30:...|NO_QUERY|           erks|is alone downstai...|is alone downstai...|    0|[is, alone, downs...|[alone, downstair...|[-0.0713094857831...|\n",
      "|              0|1467856426|Mon Apr 06 22:31:...|NO_QUERY|      AthenaA42|Grrr.. my ipods a...|Grrr my ipods act...|    0|[grrr, my, ipods,...|[grrr, ipods, act...|[0.03460237616673...|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_vectorised.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_data_vectorised = train_labelled_StringIdx.transform(test_data_vectorised)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_data_vectorised.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment_value',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " 'tweet_content',\n",
       " 'cleaned_tweet',\n",
       " 'label',\n",
       " 'tokenized_tweet',\n",
       " 'cleaned_tokens',\n",
       " 'features']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_vectorised.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "model_lr = lr.fit(train_data_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction = model_lr.transform(test_data_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|sentiment_value|       _c1|                 _c2|     _c3|            _c4|       tweet_content|       cleaned_tweet|label|     tokenized_tweet|      cleaned_tokens|            features|       rawPrediction|         probability|prediction|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|              0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...| hey long time no...|    0|[, hey, long, tim...|[, hey, long, tim...|[0.05560346074903...|[-0.5610878529843...|[0.36329578891170...|       1.0|\n",
      "|              0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...| I couldn t bear ...|    0|[, i, couldn, t, ...|[, couldn, bear, ...|[-0.0161670572124...|[1.97295653056637...|[0.87792831930739...|       0.0|\n",
      "|              0|1467819812|Mon Apr 06 22:22:...|NO_QUERY|      IrisJumbe|Oh man...was iron...|Oh man was ironin...|    0|[oh, man, was, ir...|[oh, man, ironing...|[0.01970249810256...|[-0.8912347647744...|[0.29085508106901...|       1.0|\n",
      "|              0|1467821715|Mon Apr 06 22:22:...|NO_QUERY|         deelau|@andywana Not sur...| Not sure what th...|    0|[, not, sure, wha...|[, sure, pos, muc...|[0.03242030080694...|[0.60232840362207...|[0.64618882743656...|       0.0|\n",
      "|              0|1467822384|Mon Apr 06 22:22:...|NO_QUERY|    Lindsey0920|@oanhLove I hate ...| I hate when that...|    0|[, i, hate, when,...|   [, hate, happens]|[-0.1298839425047...|[2.32798440489947...|[0.91116832840511...|       0.0|\n",
      "|              0|1467822389|Mon Apr 06 22:22:...|NO_QUERY|     HybridMink|I have a sad feel...|I have a sad feel...|    0|[i, have, a, sad,...|[sad, feeling, da...|[0.02776496097600...|[0.51600440773385...|[0.62621298955510...|       0.0|\n",
      "|              0|1467822519|Mon Apr 06 22:22:...|NO_QUERY|        gzacher|Ugh....92 degrees...|Ugh degrees tomor...|    0|[ugh, degrees, to...|[ugh, degrees, to...|[0.04406802263110...|[3.73747280065438...|[0.97673971491827...|       0.0|\n",
      "|              0|1467825883|Mon Apr 06 22:23:...|NO_QUERY|         deelau|Gym attire today ...|Gym attire today ...|    0|[gym, attire, tod...|[gym, attire, tod...|[0.00561676215147...|[-0.7357758536495...|[0.32392853801912...|       1.0|\n",
      "|              0|1467835577|Mon Apr 06 22:26:...|NO_QUERY|      viviana09|wednesday my b-da...|wednesday my b da...|    0|[wednesday, my, b...|[wednesday, b, da...|[0.16895569209009...|[0.82710381023232...|[0.69574219770607...|       0.0|\n",
      "|              0|1467836111|Mon Apr 06 22:26:...|NO_QUERY|    perrohunter|@makeherfamous hm...| hmm do u really ...|    0|[, hmm, do, u, re...|[, hmm, u, really...|[0.05667676181138...|[-0.2012919323776...|[0.44984624864415...|       1.0|\n",
      "|              0|1467837602|Mon Apr 06 22:26:...|NO_QUERY|        GetGary|@paradisej cool, ...| cool i will thei...|    0|[, cool, i, will,...|[, cool, kinds, c...|[0.01303193159401...|[0.20459454879623...|[0.55097096195877...|       0.0|\n",
      "|              0|1467839007|Mon Apr 06 22:27:...|NO_QUERY|         eyezup|@mercedesashley D...| Damn The grind i...|    0|[, damn, the, gri...|[, damn, grind, i...|[0.04390850176031...|[0.12595650425060...|[0.53144756064886...|       0.0|\n",
      "|              0|1467839450|Mon Apr 06 22:27:...|NO_QUERY|  BreannaBonana|ugh. cant sleep. ...|ugh cant sleep it...|    0|[ugh, cant, sleep...|  [ugh, cant, sleep]|[-0.0362304846445...|[3.11222452082038...|[0.95739418760769...|       0.0|\n",
      "|              0|1467839586|Mon Apr 06 22:27:...|NO_QUERY|      sonyolmos|@eRRe_sC aaw i mi...| aaw i miss ya al...|    0|[, aaw, i, miss, ...|[, aaw, miss, ya,...|[0.07940015490902...|[-0.7007181706957...|[0.33165301938061...|       1.0|\n",
      "|              0|1467841885|Mon Apr 06 22:27:...|NO_QUERY|    MissPassion|@thecoolestout Eh...| Ehhh don t Weath...|    0|[, ehhh, don, t, ...|[, ehhh, weather,...|[0.06918735057115...|[0.86158150880918...|[0.70299097092123...|       0.0|\n",
      "|              0|1467842568|Mon Apr 06 22:28:...|NO_QUERY|josiahmcdermott|i miss kenny powers |i miss kenny powers |    0|[i, miss, kenny, ...|[miss, kenny, pow...|[0.00623510622729...|[1.13292927572626...|[0.75637908134264...|       0.0|\n",
      "|              0|1467843734|Mon Apr 06 22:28:...|NO_QUERY|      chatpataa|my nokia 1110 die...|      my nokia died |    0|   [my, nokia, died]|       [nokia, died]|[0.03055888414382...|[5.66753141960599...|[0.99655551833922...|       0.0|\n",
      "|              0|1467844540|Mon Apr 06 22:28:...|NO_QUERY|      ceironous|HEROES just isn't...|HEROES just isn t...|    0|[heroes, just, is...|[heroes, isn, sea...|[-0.2572818398475...|[0.42454085637948...|[0.60456932469844...|       0.0|\n",
      "|              0|1467853431|Mon Apr 06 22:30:...|NO_QUERY|           erks|is alone downstai...|is alone downstai...|    0|[is, alone, downs...|[alone, downstair...|[-0.0713094857831...|[3.17386826277423...|[0.95983896438856...|       0.0|\n",
      "|              0|1467856426|Mon Apr 06 22:31:...|NO_QUERY|      AthenaA42|Grrr.. my ipods a...|Grrr my ipods act...|    0|[grrr, my, ipods,...|[grrr, ipods, act...|[0.03460237616673...|[0.93908115272025...|[0.71891401718772...|       0.0|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment_value: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- tweet_content: string (nullable = true)\n",
      " |-- cleaned_tweet: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- tokenized_tweet: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cleaned_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|       cleaned_tweet|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "| hey long time no...|[0.05560346074903...|    0|[-0.5610878529843...|[0.36329578891170...|       1.0|\n",
      "| I couldn t bear ...|[-0.0161670572124...|    0|[1.97295653056637...|[0.87792831930739...|       0.0|\n",
      "|Oh man was ironin...|[0.01970249810256...|    0|[-0.8912347647744...|[0.29085508106901...|       1.0|\n",
      "| Not sure what th...|[0.03242030080694...|    0|[0.60232840362207...|[0.64618882743656...|       0.0|\n",
      "| I hate when that...|[-0.1298839425047...|    0|[2.32798440489947...|[0.91116832840511...|       0.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_prediction.select(['cleaned_tweet','features','label','rawPrediction','probability','prediction']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #Creating a randomforest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "model_rf = rf.fit(train_data_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prediction = model_rf.transform(test_data_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|sentiment_value|       _c1|                 _c2|     _c3|            _c4|       tweet_content|       cleaned_tweet|label|     tokenized_tweet|      cleaned_tokens|            features|       rawPrediction|         probability|prediction|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|              0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...| hey long time no...|    0|[, hey, long, tim...|[, hey, long, tim...|[0.05560346074903...|[10.5454892478080...|[0.52727446239040...|       0.0|\n",
      "|              0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...| I couldn t bear ...|    0|[, i, couldn, t, ...|[, couldn, bear, ...|[-0.0161670572124...|[9.50408600097941...|[0.47520430004897...|       1.0|\n",
      "|              0|1467819812|Mon Apr 06 22:22:...|NO_QUERY|      IrisJumbe|Oh man...was iron...|Oh man was ironin...|    0|[oh, man, was, ir...|[oh, man, ironing...|[0.01970249810256...|[11.4660164334713...|[0.57330082167356...|       0.0|\n",
      "|              0|1467821715|Mon Apr 06 22:22:...|NO_QUERY|         deelau|@andywana Not sur...| Not sure what th...|    0|[, not, sure, wha...|[, sure, pos, muc...|[0.03242030080694...|[10.5724233823917...|[0.52862116911958...|       0.0|\n",
      "|              0|1467822384|Mon Apr 06 22:22:...|NO_QUERY|    Lindsey0920|@oanhLove I hate ...| I hate when that...|    0|[, i, hate, when,...|   [, hate, happens]|[-0.1298839425047...|[12.1063624571341...|[0.60531812285670...|       0.0|\n",
      "|              0|1467822389|Mon Apr 06 22:22:...|NO_QUERY|     HybridMink|I have a sad feel...|I have a sad feel...|    0|[i, have, a, sad,...|[sad, feeling, da...|[0.02776496097600...|[10.3635277197231...|[0.51817638598615...|       0.0|\n",
      "|              0|1467822519|Mon Apr 06 22:22:...|NO_QUERY|        gzacher|Ugh....92 degrees...|Ugh degrees tomor...|    0|[ugh, degrees, to...|[ugh, degrees, to...|[0.04406802263110...|[12.7059513261284...|[0.63529756630642...|       0.0|\n",
      "|              0|1467825883|Mon Apr 06 22:23:...|NO_QUERY|         deelau|Gym attire today ...|Gym attire today ...|    0|[gym, attire, tod...|[gym, attire, tod...|[0.00561676215147...|[10.8635047760652...|[0.54317523880326...|       0.0|\n",
      "|              0|1467835577|Mon Apr 06 22:26:...|NO_QUERY|      viviana09|wednesday my b-da...|wednesday my b da...|    0|[wednesday, my, b...|[wednesday, b, da...|[0.16895569209009...|[9.99913870653307...|[0.49995693532665...|       1.0|\n",
      "|              0|1467836111|Mon Apr 06 22:26:...|NO_QUERY|    perrohunter|@makeherfamous hm...| hmm do u really ...|    0|[, hmm, do, u, re...|[, hmm, u, really...|[0.05667676181138...|[10.5235306994010...|[0.52617653497005...|       0.0|\n",
      "|              0|1467837602|Mon Apr 06 22:26:...|NO_QUERY|        GetGary|@paradisej cool, ...| cool i will thei...|    0|[, cool, i, will,...|[, cool, kinds, c...|[0.01303193159401...|[11.0236942022509...|[0.55118471011254...|       0.0|\n",
      "|              0|1467839007|Mon Apr 06 22:27:...|NO_QUERY|         eyezup|@mercedesashley D...| Damn The grind i...|    0|[, damn, the, gri...|[, damn, grind, i...|[0.04390850176031...|[10.8579775518553...|[0.54289887759276...|       0.0|\n",
      "|              0|1467839450|Mon Apr 06 22:27:...|NO_QUERY|  BreannaBonana|ugh. cant sleep. ...|ugh cant sleep it...|    0|[ugh, cant, sleep...|  [ugh, cant, sleep]|[-0.0362304846445...|[13.2473325893917...|[0.66236662946958...|       0.0|\n",
      "|              0|1467839586|Mon Apr 06 22:27:...|NO_QUERY|      sonyolmos|@eRRe_sC aaw i mi...| aaw i miss ya al...|    0|[, aaw, i, miss, ...|[, aaw, miss, ya,...|[0.07940015490902...|[9.17334007640439...|[0.45866700382021...|       1.0|\n",
      "|              0|1467841885|Mon Apr 06 22:27:...|NO_QUERY|    MissPassion|@thecoolestout Eh...| Ehhh don t Weath...|    0|[, ehhh, don, t, ...|[, ehhh, weather,...|[0.06918735057115...|[11.1318148799415...|[0.55659074399707...|       0.0|\n",
      "|              0|1467842568|Mon Apr 06 22:28:...|NO_QUERY|josiahmcdermott|i miss kenny powers |i miss kenny powers |    0|[i, miss, kenny, ...|[miss, kenny, pow...|[0.00623510622729...|[10.5033536604531...|[0.52516768302265...|       0.0|\n",
      "|              0|1467843734|Mon Apr 06 22:28:...|NO_QUERY|      chatpataa|my nokia 1110 die...|      my nokia died |    0|   [my, nokia, died]|       [nokia, died]|[0.03055888414382...|[11.7408856861025...|[0.58704428430512...|       0.0|\n",
      "|              0|1467844540|Mon Apr 06 22:28:...|NO_QUERY|      ceironous|HEROES just isn't...|HEROES just isn t...|    0|[heroes, just, is...|[heroes, isn, sea...|[-0.2572818398475...|[9.34528670641837...|[0.46726433532091...|       1.0|\n",
      "|              0|1467853431|Mon Apr 06 22:30:...|NO_QUERY|           erks|is alone downstai...|is alone downstai...|    0|[is, alone, downs...|[alone, downstair...|[-0.0713094857831...|[12.4811107927003...|[0.62405553963501...|       0.0|\n",
      "|              0|1467856426|Mon Apr 06 22:31:...|NO_QUERY|      AthenaA42|Grrr.. my ipods a...|Grrr my ipods act...|    0|[grrr, my, ipods,...|[grrr, ipods, act...|[0.03460237616673...|[11.7821123179312...|[0.58910561589656...|       0.0|\n",
      "+---------------+----------+--------------------+--------+---------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|       cleaned_tweet|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "| hey long time no...|[0.05560346074903...|    0|[10.5454892478080...|[0.52727446239040...|       0.0|\n",
      "| I couldn t bear ...|[-0.0161670572124...|    0|[9.50408600097941...|[0.47520430004897...|       1.0|\n",
      "|Oh man was ironin...|[0.01970249810256...|    0|[11.4660164334713...|[0.57330082167356...|       0.0|\n",
      "| Not sure what th...|[0.03242030080694...|    0|[10.5724233823917...|[0.52862116911958...|       0.0|\n",
      "| I hate when that...|[-0.1298839425047...|    0|[12.1063624571341...|[0.60531812285670...|       0.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_prediction.select(['cleaned_tweet','features','label','rawPrediction','probability','prediction']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-Accuracy of logistic classifier model at predicting sentiment is: 0.7777\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "roc_accuracy=evaluator.evaluate(lr_prediction)\n",
    "print('ROC-Accuracy of logistic classifier model at predicting sentiment is: {:.4f}'.format(roc_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic classifier model at predicting sentiment is: 0.7047\n"
     ]
    }
   ],
   "source": [
    "accuracy = (lr_prediction.filter(lr_prediction['label'] == lr_prediction['prediction']).count())/(test_data.select([\"sentiment_value\"]).count())\n",
    "print('Accuracy of logistic classifier model at predicting sentiment is: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-Accuracy of random forest classifier model at predicting sentiment is: 0.7206\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "roc_accuracy=evaluator.evaluate(rf_prediction)\n",
    "print('ROC-Accuracy of random forest classifier model at predicting sentiment is: {:.4f}'.format(roc_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier model at predicting sentiment is: 0.6570\n"
     ]
    }
   ],
   "source": [
    "accuracy = rf_prediction.filter(rf_prediction['label'] == rf_prediction['prediction']).count()/test_data.select([\"sentiment_value\"]).count()\n",
    "print('Accuracy of random forest classifier model at predicting sentiment is: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the accouracy scores, I conclude that the logistic classifier  is better than random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr.save(\"Sentiment_analyser.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
